---
title: "ST_03_Variable Extraction"
author: "Matthew Coghill"
format: html
editor: visual
---

# Variable Extraction

The overarching goal of this script is to take the predictor layers generated from the ST_02 script together with the field collected data and extract the data from those predictor layers. This dataset will be what is used for modelling later on.

First, load the libraries that will be used for this script.

```{r init, include=FALSE}

ls <- c("tidyverse", "readxl", "terra", "sf")

invisible(suppressPackageStartupMessages(
  lapply(ls, library, character.only = TRUE)))
rm(ls)

# Set file access either over SFTP connection, Local LAN, or through the 
# Synology Drive app (i.e.: "SFTP", "Local", "SynologyDrive", or whatever the
# folder name is for your Synology Drive app location)
serv_conn <- "Synology"

```

## Load field data, points, and air photo points

Next, we will initialize the folders and files used in the data extraction itself. Data wrangling will be performed such that we have a single file for performing data extractions in the following chunk.

```{r load_data, include=FALSE}

# Determine server directory method (change username if using SFTP connection)
if(serv_conn == "SFTP") {
  user <- "mcoghill"
  serv <- paste0("//", user, "@aurorauav.synology.me/Cheatgrass")
} else if(serv_conn == "Local") {
  serv <- "//AuroraNAS/Cheatgrass"
} else if(serv_conn == "Synology") {
  serv <- file.path(Sys.getenv("USERPROFILE"), "SynologyDrive/Cheatgrass")
} else serv <- serv_conn

# Get the field data files (note: looking to move this to a different folder 
# at a later date, so change this as needed):
proj <- "South Thompson"
fd_dir <- file.path(serv, "Vegetation Data - 2024")
air_photo_p <- file.path(fd_dir, "Air Photo Interpretation.gpkg")
f_data_p <- file.path(fd_dir, "Veg Survey data - 2024.xlsx")
f_pts_p <- list.files(file.path(fd_dir, "Avenza Exports"),
                    pattern = ".kml$", full.names = TRUE)

air_photo <- st_read(air_photo_p, quiet = TRUE) %>% 
  janitor::clean_names() %>% 
  select(-notes)

f_data <- read_excel(f_data_p, skip = 2, col_types = "guess") %>% 
  janitor::clean_names() %>% 
  drop_na(block) %>% 
  select(block, avenza_plot_number, cheatgrass) %>% 
  rename(plot = avenza_plot_number, cover = cheatgrass) %>% 
  mutate(cover = replace_na(cover, 0))

f_pts <- do.call(rbind, lapply(f_pts_p, function(x) {
  
  if(nrow(st_layers(x)) > 1) {
    kml1 <- st_read(x, layer = st_layers(x)[2, "name"], quiet = TRUE) %>% 
      janitor::clean_names() %>% 
      select(-description) %>% 
      rename(plot_id = name) %>% 
      mutate(plot_id = gsub("Block |Blk ", "", plot_id), 
             plot_id = gsub(" - ", "_", plot_id),
             plot_id = gsub("_P", "_", plot_id))
    clean <- cbind(st_drop_geometry(kml1), 
                   as.data.frame(st_coordinates(kml1))) %>% 
      filter(Z > 0)
    kml2 <- st_read(x, layer = st_layers(x)[3, "name"], quiet = TRUE) %>% 
      janitor::clean_names() %>% 
      select(-description) %>% 
      rename(plot_id = name) %>% 
      filter(!grepl("Add", plot_id)) %>% 
      mutate(plot_id = gsub("Block |Blk ", "", plot_id), 
             plot_id = gsub(" - ", "_", plot_id),
             plot_id = gsub("_P", "_", plot_id))
    kml <- rbind(
      kml1 %>% filter(plot_id %in% clean$plot_id),
      kml2)
  } else {
    kml <- st_read(x, quiet = TRUE) %>% 
      janitor::clean_names() %>% 
      select(-description) %>% 
      rename(plot_id = name) %>% 
      filter(startsWith(plot_id, "Block")) %>% 
      mutate(plot_id = gsub("Block ", "", plot_id),
             plot_id = gsub("_P", "_", plot_id))
  }
  if(!st_geometry_type(kml, by_geometry = FALSE) == "POINT") {
    kml <- kml %>% st_collection_extract("POINT")
  }
  
  clean <- cbind(st_drop_geometry(kml), as.data.frame(st_coordinates(kml)))
  if(nrow(st_layers(x)) == 1) clean <- filter(clean, Z > 0)
  kml_clean <- kml %>% 
    filter(plot_id %in% clean$plot_id) %>% 
    separate_wider_delim(plot_id, "_", names = c("block", "plot")) %>% 
    st_as_sf() %>% 
    mutate(across(c(block, plot), as.numeric),
      surveyor = gsub(
      "AuroraUAV_2024_SamplePts_", "", gsub(".kml", "", basename(x))), 
      .after = plot) %>% 
    st_zm() %>% 
    st_transform(st_crs(air_photo))
}))

# Bind field data to field points:
fd <- left_join(f_pts, f_data, by = c("block", "plot")) %>% 
  relocate(block, plot, surveyor, cover) %>% 
  mutate(presence = cover > 0, .after = cover)

# Missing data - field data present but no associated point
# Assume missing point data came from Grant
missing_fpts <- left_join(f_data, f_pts, by = c("block", "plot")) %>% 
  st_drop_geometry() %>% 
  select(-geometry) %>% 
  filter(is.na(surveyor)) %>% 
  mutate(plot_id = paste0(block, "_", plot),
         surveyor = "Grant",
         presence = cover > 0, .after = cover)

fd_missing <- st_read(f_pts_p[1], quiet = TRUE) %>% 
  janitor::clean_names() %>% 
  select(-description) %>% 
  rename(plot_id = name) %>% 
  filter(startsWith(plot_id, "Block")) %>% 
  mutate(plot_id = gsub("Block ", "", plot_id),
         plot_id = gsub("_P", "_", plot_id)) %>% 
  filter(plot_id %in% missing_fpts$plot_id) %>% 
  st_zm() %>% 
  st_transform(st_crs(air_photo))

fd_missing_join <- left_join(missing_fpts, fd_missing, by = "plot_id") %>% 
  select(-plot_id) %>% 
  st_sf()

fd <- rbind(fd, fd_missing_join) %>% 
  arrange(block, plot)

# Missing data - no field data, but there is a point from that survey
missing_fdata <- filter(fd, is.na(cover)) %>% 
  st_drop_geometry()
write.csv(missing_fdata, file.path(fd_dir, "missing.csv"), row.names = FALSE)

# Create a single large dataset for raster extraction in next chunk
fd_full <- rbind(
  fd %>% select(cover, presence) %>% filter(!is.na(cover)),
  air_photo %>% mutate(cover = NA)) %>% vect()

```

## Raster extraction

Next, we will extract all of the covariate raster data at each of the points. At this stage, we also need to determine which raster layers to use and which ones will be omitted from modelling altogether. Once raster data is extracted it will be separated into two datasets: one for the cover data, and another for the presence/absence data. These can be saved in the same geopackage file to be pulled in during the modelling script.

```{r raster_extraction, include=FALSE}

layr_dir <- file.path(serv, "Modelling/Layers", proj)
cov_layrs <- list.files(layr_dir, pattern = ".tif$", full.names = TRUE)
cov_layrs <- cov_layrs[grep("Climate_|MS_|Terrain_", basename(cov_layrs))]
covs <- rast(cov_layrs)

# Determine layers to remove: ones with the same min and max value
covs_data <- minmax(covs, compute = TRUE)
covs <- covs[[which(covs_data[2, ] - covs_data[1, ] > 0)]]

# Determine layers to remove: ones with more NA data compared to other layers
covs_no_na <- global(covs, fun = "isNA") %>% 
  as.data.frame() %>% 
  rownames_to_column("layer") %>% 
  filter(isNA > 0) %>% 
  filter(isNA < quantile(isNA, prob = 0.98)) %>% 
  pull(layer)
covs <- covs[[covs_no_na]]

# Perform data extraction
cov_extract <- extract(covs, fd_full, bind = TRUE) %>% 
  na.omit(field = names(covs)) %>% 
  st_as_sf()

# Separate datasets for the different models
cheat_pres <- select(cov_extract, -cover)
cheat_cover <- filter(cov_extract, !is.na(cover)) %>% select(-presence)

# Write outputs - these will get used in the next script.
fd_out <- file.path(serv, "Modelling/Field Data", proj, paste0(
  proj, "_modelling_data.gpkg"))
del <- file.exists(fd_out)
st_write(cheat_pres, fd_out, layer = "presence", 
         delete_dsn = FALSE, delete_layer = del, quiet = TRUE)
st_write(cheat_cover, fd_out, layer = "cover", 
         delete_dsn = FALSE, delete_layer = del, quiet = TRUE)

```
